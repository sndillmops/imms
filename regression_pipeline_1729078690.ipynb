{"nbformat": 4, "nbformat_minor": 2, "metadata": {}, "cells": [{"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_preparation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install pandas torch scikit-learn requests imms_log_by_format\n\n", "\n\n", "import pandas as pd\n\n", "import torch\n\n", "from sklearn.model_selection import train_test_split\n\n", "import requests\n\n", "from io import StringIO\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Constants\n\n", "DATASET_URL = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-dataset-dev-local/auto_mpg_dataset.csv'\n\n", "UPLOAD_URL = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets'\n\n", "TOKEN = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "HEADERS = {'Authorization': f'Bearer {TOKEN}'}\n\n", "\n\n", "# Logger configuration\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '10'\n\n", "run_name = 'regression_pipeline_3_1_10_agent_preparation'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "\n\n", "# Load dataset\n\n", "response = requests.get(DATASET_URL, headers=HEADERS)\n\n", "response.raise_for_status()\n\n", "data = pd.read_csv(StringIO(response.text))\n\n", "\n\n", "# Preprocess dataset\n\n", "data = data.dropna()  # Drop rows with missing values\n\n", "data = data.select_dtypes(include=[float, int])  # Drop categorical columns\n\n", "\n\n", "# Split dataset\n\n", "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n", "\n\n", "# Convert datasets to CSV format in memory\n\n", "train_csv = train_data.to_csv(index=False)\n\n", "test_csv = test_data.to_csv(index=False)\n\n", "\n\n", "# Upload datasets\n\n", "def upload_dataset(csv_data, filename):\n\n", "    response = requests.put(f'{UPLOAD_URL}/{filename}', headers=HEADERS, data=csv_data)\n\n", "    response.raise_for_status()\n\n", "\n\n", "upload_dataset(train_csv, 'llmops_train_set.csv')\n\n", "upload_dataset(test_csv, 'llmops_test_set.csv')\n\n", "\n\n", "# Log the process\n\n", "logger = Logger()\n\n", "data = {\n\n", "    'status': 'success',\n\n", "    'message': 'Datasets processed and uploaded successfully'\n\n", "}\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "except Exception as e:\n\n", "    print(f\"Logging failed: {e}\")\n\n", "\n\n", "# Print confirmation\n\n", "print('datasets pushed to the hub')\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_model_download"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch requests\n\n", "import torch\n\n", "import requests\n\n", "import os\n\n", "import time\n\n", "\n\n", "# Step 1: Clear the CUDA cache\n\n", "torch.cuda.empty_cache()\n\n", "\n\n", "# Step 2: List the files in the specified folder\n\n", "api_url = \"https://artifactory.engine.capgemini.com/artifactory/api/storage/IMMS-model-dev-local/google/flant5-large?list&deep=1&listFolders=0\"\n\n", "token = \"AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q\"\n\n", "headers = {\"Authorization\": f\"Bearer {token}\"}\n\n", "\n\n", "response = requests.get(api_url, headers=headers)\n\n", "response.raise_for_status()\n\n", "file_list = response.json()['files']\n\n", "\n\n", "# Step 3: Download all the listed files and save them in ./t5-translation\n\n", "os.makedirs('./t5-translation', exist_ok=True)\n\n", "\n\n", "start_time = time.time()\n\n", "\n\n", "for file_info in file_list:\n\n", "    file_path = file_info['uri']\n\n", "    file_url = f\"https://artifactory.engine.capgemini.com/artifactory/IMMS-model-dev-local/google/flant5-large{file_path}\"\n\n", "    file_name = os.path.basename(file_path)\n\n", "    file_response = requests.get(file_url, headers=headers)\n\n", "    file_response.raise_for_status()\n\n", "    \n\n", "    with open(f'./t5-translation/{file_name}', 'wb') as file:\n\n", "        file.write(file_response.content)\n\n", "\n\n", "end_time = time.time()\n\n", "\n\n", "# Step 4: Print the total time taken to execute the complete code\n\n", "total_time = end_time - start_time\n\n", "print(f\"Total time taken: {total_time:.2f} seconds\")\n\n", "\n\n", "# Special instructions\n\n", "pipeline_name = \"regression_pipeline\"\n\n", "pipeline_id = 3\n\n", "pipeline_version = 1\n\n", "experiment_id = 10\n\n", "run_name = \"regression_pipeline_3_1_10_agent_model_download\"\n\n", "data = {\n\n", "    \"pipeline_name\": pipeline_name,\n\n", "    \"pipeline_id\": pipeline_id,\n\n", "    \"pipeline_version\": pipeline_version,\n\n", "    \"experiment_id\": experiment_id,\n\n", "    \"run_name\": run_name\n\n", "}\n\n", "\n\n", "print(\"Pipeline data:\", data)\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_regression_finetuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install pandas scikit-learn requests\n\n", "!pip install imms_log_by_format\n\n", "\n\n", "import os\n\n", "import requests\n\n", "import pandas as pd\n\n", "from sklearn.ensemble import GradientBoostingRegressor\n\n", "from sklearn.model_selection import GridSearchCV\n\n", "import pickle\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Step 1: Download the datasets\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "headers = {'Authorization': f'Bearer {token}'}\n\n", "train_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets/llmops_train_set.csv'\n\n", "test_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets/llmops_test_set.csv'\n\n", "\n\n", "train_response = requests.get(train_url, headers=headers)\n\n", "test_response = requests.get(test_url, headers=headers)\n\n", "\n\n", "with open('llmops_train_set.csv', 'wb') as f:\n\n", "    f.write(train_response.content)\n\n", "\n\n", "with open('llmops_test_set.csv', 'wb') as f:\n\n", "    f.write(test_response.content)\n\n", "\n\n", "# Step 2: Load the datasets\n\n", "train_df = pd.read_csv('llmops_train_set.csv')\n\n", "test_df = pd.read_csv('llmops_test_set.csv')\n\n", "\n\n", "X_train = train_df.drop('mpg', axis=1)\n\n", "y_train = train_df['mpg']\n\n", "X_test = test_df.drop('mpg', axis=1)\n\n", "y_test = test_df['mpg']\n\n", "\n\n", "# Step 3: Initialize and train the model\n\n", "model = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=6)\n\n", "model.fit(X_train, y_train)\n\n", "\n\n", "# Step 4: Perform grid search\n\n", "param_grid = {\n\n", "    'n_estimators': [10, 50, 100],\n\n", "    'learning_rate': [0.01, 0.1, 0.2],\n\n", "    'max_depth': [3, 6, 9],\n\n", "    'random_state': [0, 42]\n\n", "}\n\n", "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n\n", "grid_search.fit(X_train, y_train)\n\n", "\n\n", "# Step 5: Display the best hyperparameters\n\n", "best_params = grid_search.best_params_\n\n", "print(\"Best hyperparameters found by grid search:\", best_params)\n\n", "\n\n", "# Step 6: Save the fine-tuned model\n\n", "fine_tuned_model = grid_search.best_estimator_\n\n", "os.makedirs('./results', exist_ok=True)\n\n", "with open('./results/llmops_finetuned_model.pkl', 'wb') as f:\n\n", "    pickle.dump(fine_tuned_model, f)\n\n", "\n\n", "print(\"model saved locally\")\n\n", "\n\n", "# Step 7: Log the results\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '10'\n\n", "run_name = 'regression_pipeline_3_1_10_agent_regression_finetuning'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "data = {\n\n", "    'best_params': best_params,\n\n", "    'model_type': 'GradientBoostingRegressor'\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "except Exception as e:\n\n", "    print(f\"Error logging to database: {e}\")\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_model_evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install pandas requests scikit-learn joblib\n\n", "!pip install imms_log_by_format\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n\n", "import pandas as pd\n\n", "import requests\n\n", "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n", "import joblib\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Step 1: Load the dataset\n\n", "url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets/llmops_test_set.csv'\n\n", "headers = {'X-JFrog-Art-Api': 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'}\n\n", "response = requests.get(url, headers=headers)\n\n", "\n\n", "if response.status_code == 401:\n\n", "    print(\"Unauthorized access. Please check your token.\")\n\n", "    exit(1)\n\n", "elif response.status_code != 200:\n\n", "    print(f\"Failed to download the dataset. HTTP Status Code: {response.status_code}\")\n\n", "    exit(1)\n\n", "\n\n", "with open('llmops_test_set.csv', 'wb') as file:\n\n", "    file.write(response.content)\n\n", "\n\n", "df = pd.read_csv('llmops_test_set.csv')\n\n", "\n\n", "# Step 2: Load the pre-trained model\n\n", "model_path = './translation/bert_model.pkl'\n\n", "model_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-test/bert/bert_model.pkl'\n\n", "\n\n", "if not os.path.exists(model_path):\n\n", "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n\n", "    model_response = requests.get(model_url, headers=headers)\n\n", "    if model_response.status_code == 401:\n\n", "        print(\"Unauthorized access. Please check your token.\")\n\n", "        exit(1)\n\n", "    elif model_response.status_code != 200:\n\n", "        print(f\"Failed to download the model. HTTP Status Code: {model_response.status_code}\")\n\n", "        exit(1)\n\n", "    with open(model_path, 'wb') as model_file:\n\n", "        model_file.write(model_response.content)\n\n", "\n\n", "model = joblib.load(model_path)\n\n", "\n\n", "# Step 3: Evaluate the model\n\n", "X_test = df.drop(columns=['mpg'])\n\n", "y_test = df['mpg']\n\n", "\n\n", "y_pred = model.predict(X_test)\n\n", "\n\n", "mse = mean_squared_error(y_test, y_pred)\n\n", "mae = mean_absolute_error(y_test, y_pred)\n\n", "r2 = r2_score(y_test, y_pred)\n\n", "\n\n", "# Step 4: Make a prediction for the first row\n\n", "first_row = X_test.iloc[0].values.reshape(1, -1)\n\n", "first_row_prediction = model.predict(first_row)\n\n", "\n\n", "# Step 5: Log the results\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '10'\n\n", "run_name = 'regression_pipeline_3_1_10_agent_model_evaluation'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "\n\n", "data = {\n\n", "    'mean_squared_error': mse,\n\n", "    'mean_absolute_error': mae,\n\n", "    'r2_score': r2,\n\n", "    'first_row_prediction': first_row_prediction[0]\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "    print(f\"Model {model_path} that exists in local folder is evaluated for mean_squared_error, mean_absolute_error, r2_score.\")\n\n", "except Exception as e:\n\n", "    print(f\"Failed to log data: {e}\")\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_deployment"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install requests\n\n", "!pip install imms_log_by_format\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n\n", "import pickle\n\n", "import requests\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Step 1: Load the model\n\n", "model_path = './results/llmops_finetuned_model.pkl'\n\n", "config_path = './results/config_file.json'\n\n", "\n\n", "with open(model_path, 'rb') as model_file:\n\n", "    model = pickle.load(model_file)\n\n", "\n\n", "# Step 2: Upload the model and config file\n\n", "upload_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-test/bert/'\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "\n\n", "headers = {\n\n", "    'Authorization': f'Bearer {token}'\n\n", "}\n\n", "\n\n", "files = {\n\n", "    'model': open(model_path, 'rb'),\n\n", "    'config': open(config_path, 'rb')\n\n", "}\n\n", "\n\n", "response = requests.post(upload_url, headers=headers, files=files)\n\n", "\n\n", "if response.status_code == 201:\n\n", "    print(\"Model and config file uploaded successfully.\")\n\n", "else:\n\n", "    print(f\"Failed to upload files. Status code: {response.status_code}, Response: {response.text}\")\n\n", "\n\n", "# Step 3: Log the process\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '10'\n\n", "run_name = 'regression_pipeline_3_1_10_agent_deployment'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "data = {\n\n", "    'model_upload_status': response.status_code,\n\n", "    'response_text': response.text\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "    print(\"Logging successful.\")\n\n", "except Exception as e:\n\n", "    print(f\"Logging failed: {e}\")\n\n"]}]}