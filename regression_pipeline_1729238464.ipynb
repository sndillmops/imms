{"nbformat": 4, "nbformat_minor": 2, "metadata": {}, "cells": [{"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_preparation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install pandas torch scikit-learn requests imms-log-by-format\n\n", "import pandas as pd\n\n", "import torch\n\n", "from sklearn.model_selection import train_test_split\n\n", "import requests\n\n", "from io import StringIO\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Constants\n\n", "DATASET_URL = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-dataset-dev-local/auto_mpg_dataset.csv'\n\n", "UPLOAD_URL = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets'\n\n", "TOKEN = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "HEADERS = {'Authorization': f'Bearer {TOKEN}'}\n\n", "\n\n", "# Logger setup\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '12'\n\n", "run_name = 'regression_pipeline_3_1_12_agent_preparation'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "\n\n", "# Load dataset\n\n", "try:\n\n", "    response = requests.get(DATASET_URL, headers=HEADERS)\n\n", "    response.raise_for_status()\n\n", "    data = pd.read_csv(StringIO(response.text))\n\n", "except requests.exceptions.RequestException as e:\n\n", "    print(f'Error loading dataset: {e}')\n\n", "    exit(1)\n\n", "\n\n", "# Preprocess dataset\n\n", "data = data.dropna()  # Drop rows with missing values\n\n", "data = data.select_dtypes(include=[float, int])  # Drop categorical columns\n\n", "\n\n", "# Split dataset\n\n", "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n", "\n\n", "# Convert datasets to CSV format in memory\n\n", "train_csv = train_data.to_csv(index=False)\n\n", "test_csv = test_data.to_csv(index=False)\n\n", "\n\n", "# Upload datasets\n\n", "def upload_dataset(csv_data, filename):\n\n", "    try:\n\n", "        response = requests.put(\n\n", "            f'{UPLOAD_URL}/{filename}',\n\n", "            headers=HEADERS,\n\n", "            data=csv_data\n\n", "        )\n\n", "        response.raise_for_status()\n\n", "    except requests.exceptions.RequestException as e:\n\n", "        print(f'Error uploading {filename}: {e}')\n\n", "        exit(1)\n\n", "\n\n", "upload_dataset(train_csv, 'llmops_train_set.csv')\n\n", "upload_dataset(test_csv, 'llmops_test_set.csv')\n\n", "print('datasets pushed to the hub')\n\n", "\n\n", "# Log the process\n\n", "logger = Logger()\n\n", "data = {\n\n", "    'pipeline_name': pipeline_name,\n\n", "    'pipeline_id': pipeline_id,\n\n", "    'pipeline_version': pipeline_version,\n\n", "    'experiment_id': experiment_id,\n\n", "    'run_name': run_name,\n\n", "    'status': 'completed'\n\n", "}\n\n", "\n\n", "try:\n\n", "    logger.log_to_db(\n\n", "        pipeline_name=pipeline_name,\n\n", "        pipeline_id=pipeline_id,\n\n", "        pipeline_version=pipeline_version,\n\n", "        experiment_id=experiment_id,\n\n", "        run_name=run_name,\n\n", "        api_url=api_url,\n\n", "        data=data\n\n", "    )\n\n", "except Exception as e:\n\n", "    print(f'Error logging to database: {e}')\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_model_download"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch requests\n\n", "\n\n", "import torch\n\n", "import requests\n\n", "import os\n\n", "import time\n\n", "\n\n", "# Step 1: Clear the CUDA cache\n\n", "torch.cuda.empty_cache()\n\n", "\n\n", "# Variables as per special instructions\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = 3\n\n", "pipeline_version = 1\n\n", "experiment_id = 12\n\n", "run_name = 'regression_pipeline_3_1_12_agent_model_download'\n\n", "data = {\n\n", "    'pipeline_name': pipeline_name,\n\n", "    'pipeline_id': pipeline_id,\n\n", "    'pipeline_version': pipeline_version,\n\n", "    'experiment_id': experiment_id,\n\n", "    'run_name': run_name\n\n", "}\n\n", "\n\n", "# Step 2: List the files in the folder\n\n", "api_url = \"https://artifactory.engine.capgemini.com/artifactory/api/storage/IMMS-model-dev-local/google/flant5-large?list&deep=1&listFolders=0\"\n\n", "headers = {\n\n", "    'Authorization': 'Bearer AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "}\n\n", "\n\n", "response = requests.get(api_url, headers=headers)\n\n", "response.raise_for_status()\n\n", "file_list = response.json()['files']\n\n", "\n\n", "# Step 3: Download all the listed files\n\n", "download_dir = './t5-translation'\n\n", "os.makedirs(download_dir, exist_ok=True)\n\n", "\n\n", "start_time = time.time()\n\n", "\n\n", "for file_info in file_list:\n\n", "    file_path = file_info['uri']\n\n", "    file_url = f\"https://artifactory.engine.capgemini.com/artifactory/IMMS-model-dev-local/google/flant5-large{file_path}\"\n\n", "    file_name = os.path.basename(file_path)\n\n", "    file_response = requests.get(file_url, headers=headers)\n\n", "    file_response.raise_for_status()\n\n", "    \n\n", "    with open(os.path.join(download_dir, file_name), 'wb') as file:\n\n", "        file.write(file_response.content)\n\n", "\n\n", "# Step 4: Print the total time taken\n\n", "end_time = time.time()\n\n", "total_time = end_time - start_time\n\n", "print(f\"Total time taken: {total_time} seconds\")\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_regression_finetuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install pandas scikit-learn requests\n\n", "!pip install imms_log_by_format\n\n", "import os\n\n", "import requests\n\n", "import pandas as pd\n\n", "import pickle\n\n", "from sklearn.ensemble import GradientBoostingRegressor\n\n", "from sklearn.model_selection import GridSearchCV\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Step 1: Download the datasets\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "headers = {'Authorization': f'Bearer {token}'}\n\n", "train_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets/llmops_train_set.csv'\n\n", "test_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets/llmops_test_set.csv'\n\n", "\n\n", "train_response = requests.get(train_url, headers=headers)\n\n", "test_response = requests.get(test_url, headers=headers)\n\n", "\n\n", "with open('llmops_train_set.csv', 'wb') as f:\n\n", "    f.write(train_response.content)\n\n", "\n\n", "with open('llmops_test_set.csv', 'wb') as f:\n\n", "    f.write(test_response.content)\n\n", "\n\n", "# Step 2: Load the datasets\n\n", "train_data = pd.read_csv('llmops_train_set.csv')\n\n", "test_data = pd.read_csv('llmops_test_set.csv')\n\n", "\n\n", "# Step 3: Initialize the model\n\n", "model = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=6)\n\n", "\n\n", "# Step 4: Train the model\n\n", "X_train = train_data.drop('mpg', axis=1)\n\n", "y_train = train_data['mpg']\n\n", "model.fit(X_train, y_train)\n\n", "\n\n", "# Step 5: Perform a grid search to fine-tune the hyperparameters\n\n", "param_grid = {\n\n", "    'n_estimators': [10, 50, 100],\n\n", "    'learning_rate': [0.01, 0.1, 0.2],\n\n", "    'max_depth': [3, 6, 9],\n\n", "    'random_state': [0, 42]\n\n", "}\n\n", "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n\n", "grid_search.fit(X_train, y_train)\n\n", "\n\n", "# Step 6: Display the best hyperparameters\n\n", "best_params = grid_search.best_params_\n\n", "print(\"Best hyperparameters found by grid search:\", best_params)\n\n", "\n\n", "# Step 7: Save the fine-tuned model\n\n", "fine_tuned_model = grid_search.best_estimator_\n\n", "os.makedirs('./results', exist_ok=True)\n\n", "with open('./results/llmops_finetuned_model.pkl', 'wb') as f:\n\n", "    pickle.dump(fine_tuned_model, f)\n\n", "\n\n", "# Step 8: Log the results using the Logger class\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '12'\n\n", "run_name = 'regression_pipeline_3_1_12_agent_regression_finetuning'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "data = {\n\n", "    'best_params': best_params,\n\n", "    'model_type': 'GradientBoostingRegressor'\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "except Exception as e:\n\n", "    print(f\"Error logging to database: {e}\")\n\n", "\n\n", "# Step 9: Print the text 'model saved locally'\n\n", "print('model saved locally')\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_model_evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install pandas requests scikit-learn transformers torch\n\n", "\n\n", "import os\n\n", "import pandas as pd\n\n", "import requests\n\n", "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n", "from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n", "import torch\n\n", "from imms_log_by_format import Logger\n\n", "import numpy as np\n\n", "\n\n", "# Step 1: Load the dataset\n\n", "url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS_datasets/llmops_test_set.csv'\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "headers = {'Authorization': f'Bearer {token}'}\n\n", "response = requests.get(url, headers=headers)\n\n", "response.raise_for_status()\n\n", "\n\n", "with open('llmops_test_set.csv', 'wb') as f:\n\n", "    f.write(response.content)\n\n", "\n\n", "df = pd.read_csv('llmops_test_set.csv')\n\n", "\n\n", "# Step 2: Load the pre-trained model\n\n", "model_path = './translation'\n\n", "\n\n", "if not os.path.exists(model_path):\n\n", "    raise FileNotFoundError(f\"The model path {model_path} does not exist. Please check the path and try again.\")\n\n", "\n\n", "try:\n\n", "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n\n", "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n\n", "except Exception as e:\n\n", "    raise EnvironmentError(f\"Failed to load the model from {model_path}. Error: {e}\")\n\n", "\n\n", "# Step 3: Evaluate the model\n\n", "X_test = df.drop(columns=['mpg'])\n\n", "y_test = df['mpg']\n\n", "\n\n", "# Convert the test data to string format\n\n", "X_test_str = X_test.astype(str).apply(lambda x: ' '.join(x), axis=1)\n\n", "\n\n", "# Tokenize the test data\n\n", "inputs = tokenizer(X_test_str.tolist(), return_tensors='pt', padding=True, truncation=True)\n\n", "outputs = model.generate(**inputs, max_new_tokens=50)\n\n", "predictions = []\n\n", "\n\n", "for output in outputs:\n\n", "    decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n\n", "    try:\n\n", "        # Extract the first numerical value from the generated sequence\n\n", "        prediction = float(decoded_output.split()[0])\n\n", "    except (ValueError, IndexError):\n\n", "        prediction = float('nan')  # Handle cases where conversion fails or no tokens are generated\n\n", "    predictions.append(prediction)\n\n", "\n\n", "# Filter out NaN values\n\n", "valid_indices = ~np.isnan(predictions)\n\n", "y_test_valid = y_test[valid_indices]\n\n", "predictions_valid = np.array(predictions)[valid_indices]\n\n", "\n\n", "# Calculate metrics\n\n", "mse = mean_squared_error(y_test_valid, predictions_valid)\n\n", "mae = mean_absolute_error(y_test_valid, predictions_valid)\n\n", "r2 = r2_score(y_test_valid, predictions_valid)\n\n", "\n\n", "# Step 4: Make a prediction for the first row\n\n", "first_row = X_test_str.iloc[0]\n\n", "inputs = tokenizer(first_row, return_tensors='pt', padding=True, truncation=True)\n\n", "outputs = model.generate(**inputs, max_new_tokens=50)\n\n", "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n", "try:\n\n", "    first_row_prediction = float(decoded_output.split()[0])\n\n", "except (ValueError, IndexError):\n\n", "    first_row_prediction = float('nan')  # Handle cases where conversion fails or no tokens are generated\n\n", "\n\n", "# Step 5: Log the results\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '12'\n\n", "run_name = 'regression_pipeline_3_1_12_agent_model_evaluation'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "data = {\n\n", "    'mean_squared_error': mse,\n\n", "    'mean_absolute_error': mae,\n\n", "    'r2_score': r2,\n\n", "    'first_row_prediction': first_row_prediction\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "    print(f\"Model {model_path} that exists in local folder is evaluated for mean_squared_error, mean_absolute_error, r2_score.\")\n\n", "except Exception as e:\n\n", "    print(f\"Failed to log data: {e}\")\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_deployment"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install requests\n\n", "!pip install imms_log_by_format\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n\n", "import pickle\n\n", "import requests\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Step 1: Load the model\n\n", "model_path = './results/llmops_finetuned_model.pkl'\n\n", "with open(model_path, 'rb') as file:\n\n", "    model = pickle.load(file)\n\n", "\n\n", "# Step 2: Upload the model and config file\n\n", "upload_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-test/bert/'\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "headers = {'Authorization': f'Bearer {token}'}\n\n", "\n\n", "# Upload model\n\n", "model_file = {'file': open(model_path, 'rb')}\n\n", "response_model = requests.put(upload_url + 'llmops_finetuned_model.pkl', files=model_file, headers=headers)\n\n", "model_file['file'].close()\n\n", "\n\n", "# Check if the model upload was successful\n\n", "if response_model.status_code == 201:\n\n", "    print(\"Model uploaded successfully.\")\n\n", "else:\n\n", "    print(f\"Failed to upload model. Status code: {response_model.status_code}\")\n\n", "    print(response_model.text)\n\n", "\n\n", "# Check if config file exists\n\n", "config_path = './results/config.json'\n\n", "if os.path.exists(config_path):\n\n", "    # Upload config file\n\n", "    config_file = {'file': open(config_path, 'rb')}\n\n", "    response_config = requests.put(upload_url + 'config.json', files=config_file, headers=headers)\n\n", "    config_file['file'].close()\n\n", "\n\n", "    # Check if the config upload was successful\n\n", "    if response_config.status_code == 201:\n\n", "        print(\"Config file uploaded successfully.\")\n\n", "    else:\n\n", "        print(f\"Failed to upload config file. Status code: {response_config.status_code}\")\n\n", "        print(response_config.text)\n\n", "else:\n\n", "    print(\"Config file not found.\")\n\n", "    response_config = None\n\n", "\n\n", "# Step 3: Log the details using Logger\n\n", "pipeline_name = 'regression_pipeline'\n\n", "pipeline_id = '3'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '12'\n\n", "run_name = 'regression_pipeline_3_1_12_agent_deployment'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "data = {\n\n", "    'model_upload_status': response_model.status_code,\n\n", "    'config_upload_status': response_config.status_code if response_config else 'File not found'\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "    print(\"Logging successful.\")\n\n", "except Exception as e:\n\n", "    print(f\"Failed to log data: {e}\")\n\n"]}]}