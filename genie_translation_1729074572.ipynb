{"nbformat": 4, "nbformat_minor": 2, "metadata": {}, "cells": [{"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# translation_agent_preparation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n\n", "import requests\n\n", "import pandas as pd\n\n", "from sklearn.model_selection import train_test_split\n\n", "from io import StringIO\n\n", "import time\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Step 1: Clear the CUDA cache\n\n", "torch.cuda.empty_cache()\n\n", "\n\n", "# Step 2: Load the dataset\n\n", "url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-dataset-dev-local/wmt_100.csv'\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "headers = {'Authorization': f'Bearer {token}'}\n\n", "response = requests.get(url, headers=headers)\n\n", "response.raise_for_status()  # Raise an error if the request failed\n\n", "\n\n", "# Step 3: Read the dataset into a pandas DataFrame\n\n", "data = pd.read_csv(StringIO(response.text))\n\n", "\n\n", "# Step 4: Split the data into training and testing sets\n\n", "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n\n", "\n\n", "# Step 5: Preprocess the dataset by handling missing values\n\n", "train_data.fillna(method='ffill', inplace=True)\n\n", "test_data.fillna(method='ffill', inplace=True)\n\n", "\n\n", "# Convert DataFrames to CSV format without saving to disk\n\n", "train_csv = train_data.to_csv(index=False)\n\n", "test_csv = test_data.to_csv(index=False)\n\n", "\n\n", "# Step 6: Upload the datasets\n\n", "upload_url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-dataset-dev-local'\n\n", "# Upload train dataset\n\n", "train_response = requests.put(f'{upload_url}/llmops_train_set.csv', headers=headers, data=train_csv)\n\n", "train_response.raise_for_status()  # Raise an error if the upload failed\n\n", "\n\n", "# Upload test dataset\n\n", "test_response = requests.put(f'{upload_url}/llmops_test_set.csv', headers=headers, data=test_csv)\n\n", "test_response.raise_for_status()  # Raise an error if the upload failed\n\n", "\n\n", "# Step 7: Install required libraries\n\n", "import subprocess\n\n", "subprocess.run(['pip', 'install', 'transformers[torch]', 'SentencePiece', 'rouge_score'], check=True)\n\n", "\n\n", "# Step 8: Log the process\n\n", "pipeline_name = 'genie_translation'\n\n", "pipeline_id = '1'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '9'\n\n", "run_name = 'genie_translation_1_1_9_translation_agent_preparation'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "data = {\n\n", "    'train_dataset': 'llmops_train_set.csv',\n\n", "    'test_dataset': 'llmops_test_set.csv',\n\n", "    'status': 'uploaded'\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "\n\n", "# Step 9: Print the completion message and total time taken\n\n", "start_time = time.time()\n\n", "print('datasets pushed to the hub')\n\n", "end_time = time.time()\n\n", "print(f'Total time taken: {end_time - start_time} seconds')\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# agent_model_download"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch requests\n\n", "import torch\n\n", "import requests\n\n", "import os\n\n", "import time\n\n", "\n\n", "# Step 1: Clear the CUDA cache\n\n", "torch.cuda.empty_cache()\n\n", "\n\n", "# Step 2: List the files in the specified folder\n\n", "api_url = \"https://artifactory.engine.capgemini.com/artifactory/api/storage/IMMS-model-dev-local/google/flant5-large?list&deep=1&listFolders=0\"\n\n", "token = \"AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q\"\n\n", "headers = {\"Authorization\": f\"Bearer {token}\"}\n\n", "\n\n", "response = requests.get(api_url, headers=headers)\n\n", "response.raise_for_status()\n\n", "file_list = response.json()['files']\n\n", "\n\n", "# Step 3: Download all the listed files and save them in ./t5-translation\n\n", "os.makedirs('./t5-translation', exist_ok=True)\n\n", "\n\n", "start_time = time.time()\n\n", "\n\n", "for file_info in file_list:\n\n", "    file_path = file_info['uri']\n\n", "    file_url = f\"https://artifactory.engine.capgemini.com/artifactory/IMMS-model-dev-local/google/flant5-large{file_path}\"\n\n", "    file_name = os.path.basename(file_path)\n\n", "    file_response = requests.get(file_url, headers=headers)\n\n", "    file_response.raise_for_status()\n\n", "    \n\n", "    with open(f'./t5-translation/{file_name}', 'wb') as file:\n\n", "        file.write(file_response.content)\n\n", "\n\n", "end_time = time.time()\n\n", "\n\n", "# Step 4: Print the total time taken to execute the complete code\n\n", "total_time = end_time - start_time\n\n", "print(f\"Total time taken: {total_time} seconds\")\n\n", "\n\n", "# Special instructions: Create and assign variables\n\n", "pipeline_name = \"genie_translation\"\n\n", "pipeline_id = 1\n\n", "pipeline_version = 1\n\n", "experiment_id = 9\n\n", "run_name = \"genie_translation_1_1_9_agent_model_download\"\n\n", "data = {\n\n", "    \"pipeline_name\": pipeline_name,\n\n", "    \"pipeline_id\": pipeline_id,\n\n", "    \"pipeline_version\": pipeline_version,\n\n", "    \"experiment_id\": experiment_id,\n\n", "    \"run_name\": run_name\n\n", "}\n\n", "\n\n", "print(\"Pipeline data:\", data)\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# translation_agent_finetuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch deepspeed transformers datasets pandas requests\n\n", "!pip install imms_log_by_format\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n\n", "import torch\n\n", "import deepspeed\n\n", "import requests\n\n", "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n\n", "from datasets import Dataset\n\n", "import pandas as pd\n\n", "from imms_log_by_format import Logger\n\n", "\n\n", "# Step 1: Clear the CUDA cache\n\n", "torch.cuda.empty_cache()\n\n", "\n\n", "# Step 2: Initialize distributed training with DeepSpeed\n\n", "deepspeed.init_distributed()\n\n", "\n\n", "# Step 3: Download the datasets\n\n", "def download_dataset(url, token, filename):\n\n", "    headers = {'Authorization': f'Bearer {token}'}\n\n", "    response = requests.get(url, headers=headers)\n\n", "    response.raise_for_status()\n\n", "    with open(filename, 'wb') as f:\n\n", "        f.write(response.content)\n\n", "\n\n", "dataset_urls = {\n\n", "    'train': 'https://artifactory.engine.capgemini.com/artifactory/IMMS-dataset-dev-local/llmops_train_set.csv',\n\n", "    'test': 'https://artifactory.engine.capgemini.com/artifactory/IMMS-dataset-dev-local/llmops_test_set.csv'\n\n", "}\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "\n\n", "for name, url in dataset_urls.items():\n\n", "    download_dataset(url, token, f'{name}_set.csv')\n\n", "\n\n", "# Step 4: Load the model for fine-tuning\n\n", "model_name = './t5-translation'\n\n", "tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n", "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n", "\n\n", "# Step 5: Tokenize the input sequences and labels\n\n", "def tokenize_function(examples):\n\n", "    inputs = tokenizer(examples['en'], padding=\"max_length\", truncation=True)\n\n", "    targets = tokenizer(examples['fr'], padding=\"max_length\", truncation=True)\n\n", "    inputs['labels'] = targets['input_ids']\n\n", "    return inputs\n\n", "\n\n", "train_data = pd.read_csv('train_set.csv')\n\n", "test_data = pd.read_csv('test_set.csv')\n\n", "\n\n", "train_dataset = Dataset.from_pandas(train_data)\n\n", "test_dataset = Dataset.from_pandas(test_data)\n\n", "\n\n", "train_dataset = train_dataset.map(tokenize_function, batched=True)\n\n", "test_dataset = test_dataset.map(tokenize_function, batched=True)\n\n", "\n\n", "# Step 6: Ensure proper alignment between input data and labels\n\n", "# This is handled by the tokenize_function\n\n", "\n\n", "# Step 7: Configure DeepSpeed's zero optimization\n\n", "ds_config = {\n\n", "    \"train_batch_size\": \"auto\",\n\n", "    \"gradient_accumulation_steps\": 1,\n\n", "    \"fp16\": {\n\n", "        \"enabled\": \"auto\"\n\n", "    },\n\n", "    \"zero_optimization\": {\n\n", "        \"stage\": 2,\n\n", "        \"offload_optimizer\": {\n\n", "            \"device\": \"cpu\",\n\n", "            \"pin_memory\": True\n\n", "        },\n\n", "        \"allgather_partitions\": True,\n\n", "        \"allgather_bucket_size\": 2e8,\n\n", "        \"reduce_scatter\": True,\n\n", "        \"reduce_bucket_size\": 2e8,\n\n", "        \"overlap_comm\": True,\n\n", "        \"contiguous_gradients\": True\n\n", "    }\n\n", "}\n\n", "\n\n", "# Step 8: Handle model sharding if necessary\n\n", "# This is handled by DeepSpeed's zero optimization configuration\n\n", "\n\n", "# Step 9: Track the loss function and metrics\n\n", "training_args = TrainingArguments(\n\n", "    output_dir='./results',\n\n", "    per_device_train_batch_size=8,\n\n", "    per_device_eval_batch_size=8,\n\n", "    num_train_epochs=3,\n\n", "    logging_dir='./logs',\n\n", "    logging_steps=10,\n\n", "    deepspeed=ds_config\n\n", ")\n\n", "\n\n", "trainer = Trainer(\n\n", "    model=model,\n\n", "    args=training_args,\n\n", "    train_dataset=train_dataset,\n\n", "    eval_dataset=test_dataset\n\n", ")\n\n", "\n\n", "# Step 10: Save the fine-tuned model and tokenizer\n\n", "trainer.train()\n\n", "model.save_pretrained('./translation')\n\n", "tokenizer.save_pretrained('./translation')\n\n", "\n\n", "# Step 11: Implement logging with error handling\n\n", "pipeline_name = 'genie_translation'\n\n", "pipeline_id = '1'\n\n", "pipeline_version = '1'\n\n", "experiment_id = '9'\n\n", "run_name = 'genie_translation_1_1_9_translation_agent_finetuning'\n\n", "api_url = 'http://localhost:3290/bpfx/workspace/logs'\n\n", "data = {\n\n", "    'status': 'completed',\n\n", "    'message': 'Fine-tuning completed successfully'\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "except Exception as e:\n\n", "    print(f\"Logging failed: {e}\")\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# translation_agent_evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install torch datasets transformers pandas evaluate\n\n", "!pip install imms_log_by_format\n\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n\n", "from datasets import load_dataset\n\n", "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n", "from imms_log_by_format import Logger\n\n", "import pandas as pd\n\n", "import evaluate\n\n", "\n\n", "# Step 1: Clear the CUDA cache\n\n", "torch.cuda.empty_cache()\n\n", "\n\n", "# Step 2: Load the dataset\n\n", "dataset_path = \"llmops_test_set.csv\"\n\n", "dataset = load_dataset('csv', data_files=dataset_path, split='train')\n\n", "\n\n", "# Step 3: Tokenize the dataset\n\n", "model_name = \"./translation\"\n\n", "tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n", "\n\n", "def tokenize_function(examples):\n\n", "    return tokenizer(examples['en'], padding=\"max_length\", truncation=True)\n\n", "\n\n", "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n\n", "\n\n", "# Step 4: Load the fine-tuned model and tokenizer\n\n", "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n\n", "\n\n", "# Step 5: Generate predictions\n\n", "def generate_predictions(batch):\n\n", "    inputs = tokenizer(batch['en'], return_tensors='pt', padding=\"max_length\", truncation=True).to('cuda')\n\n", "    outputs = model.generate(inputs['input_ids'], max_length=50, num_beams=5)\n\n", "    batch['predicted'] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n", "    return batch\n\n", "\n\n", "predictions = tokenized_dataset.map(generate_predictions, batched=True, batch_size=8)\n\n", "\n\n", "# Step 6: Prefix the evaluation prompt\n\n", "predictions = predictions.map(lambda x: {\"en\": \"Translate from en to fr: \" + x['en']})\n\n", "\n\n", "# Step 7: Store the predicted and actual values in a CSV file\n\n", "predictions_df = pd.DataFrame(predictions)\n\n", "predictions_df.to_csv('predictions.csv', index=False)\n\n", "\n\n", "# Step 8: Calculate evaluation metrics\n\n", "bleu = evaluate.load('bleu')\n\n", "rouge = evaluate.load('rouge')\n\n", "\n\n", "references = [ref for ref in predictions['fr']]\n\n", "predictions_list = [pred for pred in predictions['predicted']]\n\n", "\n\n", "bleu_score = bleu.compute(predictions=predictions_list, references=references)\n\n", "rouge_score = rouge.compute(predictions=predictions_list, references=references)\n\n", "\n\n", "# Step 9: Log the evaluation results\n\n", "pipeline_name = \"genie_translation\"\n\n", "pipeline_id = \"1\"\n\n", "pipeline_version = \"1\"\n\n", "experiment_id = \"9\"\n\n", "run_name = \"genie_translation_1_1_9_translation_agent_evaluation\"\n\n", "api_url = \"http://localhost:3290/bpfx/workspace/logs\"\n\n", "\n\n", "data = {\n\n", "    \"BLEU\": bleu_score,\n\n", "    \"ROUGE\": rouge_score\n\n", "}\n\n", "\n\n", "logger = Logger()\n\n", "try:\n\n", "    logger.log_to_db(pipeline_name, pipeline_id, pipeline_version, experiment_id, run_name, api_url, data)\n\n", "except Exception as e:\n\n", "    print(f\"Logging failed: {e}\")\n\n", "\n\n", "# Step 10: Print both the actual and predicted results\n\n", "print(predictions_df[['en', 'fr', 'predicted']])\n\n"]}, {"cell_type": "markdown", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# translation_agent_deployment"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install requests\n\n", "\n\n", "import os\n\n", "import requests\n\n", "import time\n\n", "\n\n", "# Set up the necessary variables\n\n", "pipeline_name = 'genie_translation'\n\n", "pipeline_id = 1\n\n", "pipeline_version = 1\n\n", "experiment_id = 9\n\n", "run_name = 'genie_translation_1_1_9_translation_agent_deployment'\n\n", "\n\n", "data = {\n\n", "    'pipeline_name': pipeline_name,\n\n", "    'pipeline_id': pipeline_id,\n\n", "    'pipeline_version': pipeline_version,\n\n", "    'experiment_id': experiment_id,\n\n", "    'run_name': run_name\n\n", "}\n\n", "\n\n", "# URL and token\n\n", "url = 'https://artifactory.engine.capgemini.com/artifactory/IMMS-model-dev-local/google/flant5-large-v1'\n\n", "token = 'AKCpBtMeFndD5dudesorJSq64URz2WPtU3jfW7DqLwfDyD51vtneZkih6yNrFugBmxKgyFQ9q'\n\n", "\n\n", "# Directory containing the model files\n\n", "directory = './translation'\n\n", "\n\n", "# Start the timer\n\n", "start_time = time.time()\n\n", "\n\n", "# Iterate through the files in the directory and upload each one\n\n", "for filename in os.listdir(directory):\n\n", "    file_path = os.path.join(directory, filename)\n\n", "    if os.path.isfile(file_path):\n\n", "        with open(file_path, 'rb') as f:\n\n", "            response = requests.put(\n\n", "                f\"{url}/{filename}\",\n\n", "                headers={'Authorization': f'Bearer {token}'},\n\n", "                data=f\n\n", "            )\n\n", "            if response.status_code == 201:\n\n", "                print(f\"Successfully uploaded {filename}\")\n\n", "            else:\n\n", "                print(f\"Failed to upload {filename}. Status code: {response.status_code}\")\n\n", "\n\n", "# End the timer\n\n", "end_time = time.time()\n\n", "\n\n", "# Calculate the total time taken\n\n", "total_time = end_time - start_time\n\n", "\n\n", "# Print the model uploaded to hub and the total time taken\n\n", "print(f\"Model uploaded to hub: {url}\")\n\n", "print(f\"Total time taken: {total_time} seconds\")\n\n"]}]}